{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbd3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6413a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Import the OpenCV library for video processing\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Initialize an empty list to store frames\n",
    "    frames = []\n",
    "    \n",
    "    # Loop until the video file is open\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Check if the frame was read successfully\n",
    "        if not ret:\n",
    "            # Exit the loop if no more frames are available\n",
    "            break\n",
    "        \n",
    "        # Append the frame to the list of frames\n",
    "        frames.append(frame)\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    # Return the list of frames\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2700dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def run_yolo_detection(frame, net, output_layers):\n",
    "    # Get the dimensions of the frame\n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # Prepare the frame for the YOLO model\n",
    "    # Convert the image to a blob and normalize\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    \n",
    "    # Set the input for the YOLO model\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Perform forward pass and get output from the output layers\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "    \n",
    "    # Initialize an empty list to store detected boxes\n",
    "    boxes = []\n",
    "    \n",
    "    # Process each output layer\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            # Extract class scores and class ID\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            # Check if confidence is above the threshold\n",
    "            if confidence > 0.5:\n",
    "                # Calculate the bounding box coordinates\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # Convert center coordinates and width/height to bounding box coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                # Append the bounding box coordinates to the list\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                \n",
    "    # Return the list of bounding boxes\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7d5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_over_frames(frames, ground_truths, net, output_layers, iou_threshold=0.5):\n",
    "    # Initialize counters for true positives and false positives\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    \n",
    "    # Iterate over each frame and its corresponding ground truths\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Run YOLO detection on the current frame\n",
    "        detections = run_yolo_detection(frame, net, output_layers)\n",
    "        \n",
    "        # Retrieve the ground truths for the current frame\n",
    "        frame_ground_truths = ground_truths[i]\n",
    "        \n",
    "        # Initialize counters for true positives and false positives for the current frame\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        \n",
    "        # Check each detected bounding box\n",
    "        for detection in detections:\n",
    "            matched = False\n",
    "            # Compare detection with each ground truth\n",
    "            for gt in frame_ground_truths:\n",
    "                # Calculate Intersection over Union (IoU) between detection and ground truth\n",
    "                iou = calculate_iou(detection, gt)\n",
    "                if iou > iou_threshold:\n",
    "                    # If IoU is above threshold, consider it a true positive\n",
    "                    true_positives += 1\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if not matched:\n",
    "                # If no match was found, consider it a false positive\n",
    "                false_positives += 1\n",
    "        \n",
    "        # Accumulate counts of true positives and false positives\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "    \n",
    "    # Calculate overall precision\n",
    "    overall_precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    \n",
    "    return overall_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb36703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_path, ground_truths):\n",
    "    # Load YOLO model\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    # Get layer names\n",
    "    layer_names = net.getLayerNames()\n",
    "\n",
    "    # Handle both 1D and 2D outputs from getUnconnectedOutLayers\n",
    "    unconnected_layers = net.getUnconnectedOutLayers()\n",
    "\n",
    "    if isinstance(unconnected_layers[0], list):\n",
    "        output_layers = [layer_names[i[0] - 1] for i in unconnected_layers]\n",
    "    else:\n",
    "        output_layers = [layer_names[i - 1] for i in unconnected_layers]\n",
    "    \n",
    "    frames = extract_frames(video_path)\n",
    "    \n",
    "    overall_precision = calculate_precision_over_frames(frames, ground_truths, net, output_layers)\n",
    "    \n",
    "    print(f\"Overall Precision: {overall_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205a5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '4K Road traffic video for object detection and tracking - free download now!.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63592d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
